\relax 
\@writefile{toc}{\contentsline {title}{Real-time Power Prediction in Tour de France}{1}}
\@writefile{toc}{\authcount {2}}
\@writefile{toc}{\contentsline {author}{ Yasuyuki Kataoka \and Peter Gray}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{ref_1}
\citation{ref_2}
\citation{ref_3}
\citation{ref_4}
\citation{ref_5}
\citation{ref_6}
\citation{ref_7}
\citation{ref_8}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Fatigue Analytics}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Machine Learning Application using cycling data}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Dataset}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Power Distribution: The imbalanced data makes hard to predict the high or low power range}}{4}}
\newlabel{fig:power_distribution}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Input Data: GPS and wind sensor}{4}}
\newlabel{sec:input_data}{{3.1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Labeled Data: Power sensor}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Machine Learning pipeline: Raw input is obtained from {\it  {Dimension Data's data analytics platform}}. Both hand-made feature by physics and automatic feature generated by Deep Learning is concatenated for Machine Learning model.}}{5}}
\newlabel{fig:machine_learning_pipeline}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Feature Engineering}{5}}
\newlabel{sec:hand_made_feature}{{4.1}{5}}
\@writefile{toc}{\contentsline {subsubsection}{Hand-made Feature}{5}}
\citation{ref_9}
\citation{ref_10}
\citation{ref_11}
\@writefile{toc}{\contentsline {subsubsection}{Generated Feature by Deep Learning}{6}}
\citation{ref_12}
\citation{ref_13}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Data normalization for GPS trajectory}}{7}}
\newlabel{fig:data_normalization}{{3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces embedded trajectory feature by autoencoder}}{7}}
\newlabel{fig:deep_autoencoder}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Regression Model}{7}}
\newlabel{sec:regression_model}{{4.2}{7}}
\citation{ref_14}
\citation{ref_15}
\@writefile{toc}{\contentsline {subsubsection}{Tree-based Models}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Time-series Deep Learning Models(Recurrent Neural Net)}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Result}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Feature Engineering}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance comparison between four different feature engineering by MAE.}}{9}}
\newlabel{tab1:MAE_result}{{1}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces An example of power prediction with the comparison to ground truth}}{9}}
\newlabel{fig:power_prediction_example}{{5}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance comparison between autoencoder Model}}{10}}
\newlabel{tab1:autoencoder_result}{{2}{10}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of embedded trajectory feature by various autoencoder}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Regression Model}{10}}
\@writefile{toc}{\contentsline {subsubsection}{Inference Latency}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Latency Comparison between different regression models (RF=Random Forest, XB=XGBoost, \_20,\_200= estimation by 20,200 trees, LSTM\_1,\_3=LSTM by 1,3 layers, GRU\_1,\_3=GRU with 1,3 layers)}}{11}}
\newlabel{fig:latency_comparison}{{6}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Eror Rate - MAE}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Stacked LSTM: validation MAE performance over epochs: }}{12}}
\newlabel{fig:stacked_lstm_training}{{7}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance Comparison between Regression Models by MAE and latency}}{12}}
\newlabel{tab1:MAE_latency_result}{{3}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Qualitative Analysis}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Real Deployment in Tour de France 2017}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Power Distribution for two group: x-axis means fatigue index, y-axis means probability, each bar chart means power distribution for two different rider groups.}}{13}}
\newlabel{fig:power_comparison_2group}{{8}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluating the impact on Fan Engagement}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Example of Social Media Exposure: Visualization of the winner's performance in accordance with the terrain variation}}{14}}
\newlabel{fig:power_prediction_graph}{{9}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{14}}
\bibcite{ref_1}{1}
\bibcite{ref_2}{2}
\bibcite{ref_3}{3}
\bibcite{ref_4}{4}
\bibcite{ref_5}{5}
\bibcite{ref_6}{6}
\bibcite{ref_7}{7}
\bibcite{ref_8}{8}
\bibcite{ref_9}{9}
\bibcite{ref_10}{10}
\bibcite{ref_11}{11}
\bibcite{ref_12}{12}
\bibcite{ref_13}{13}
\bibcite{ref_14}{14}
\bibcite{ref_15}{15}
\@writefile{toc}{\contentsline {section}{\numberline {A}Data Collection System Backend Architecture}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Real-time data streaming architecture of data collection platform that was used in {\it  {Tour de France 2017}}}}{16}}
\newlabel{fig:data_collection_platform}{{10}{16}}
